---
title: "Forecasting # of visitors from Malaysia"
output: html_notebook
---


###CMV
```{r}
source("source.R")
TS_SE1 <- read.csv("Malaysia.csv")
Visitors <- TS_SE1$Malaysia
Quarter <- TS_SE1$Month   
Season <- max(Quarter)
Period <- TS_SE1$period     
t  = Period -1               
n <- length(Visitors)

CMV_SI <- Center_Moving_Average(Visitors, Quarter, Season)
cat("# # # # The Centered moving average seasonal index # # # ","\n")
print(CMV_SI)

```


### Forecast by seasonal index
```{r}
Forecast_Visitors_CMV= Forecast_by_SI(Visitors, Quarter, Season, CMV_SI, t)

cat("# # # # The Forecast_Visitors_y # # # ","\n")
print(Forecast_Visitors_CMV)


t2 = c(t,n,n+1,n+2,n+3,n+4,n+5,n+6,n+7,n+8,n+9,n+10,n+11)
lines(t,Visitors, col = "blue", main = "Visitorsagainst Period", xlab = "Period", ylab = "Tourists")
lines(t, Visitors, type="o", col = "blue")    #actual
lines(t2,Forecast_Visitors_CMV, type="o", col = "red")  #forecast
legend("topleft", c("actual", "deseasonalized", "forecast"), fill=c("blue","green", "red"))
```

*Residual analysis: *            
  1.normality test       
  H0: Errors are normally distributed.        
  H1: Errors are not normally distributed.         
  Since pvalue = 0.5239 > 0.05, we do not reject H0.
  
  2..Homoscedasticity & Heteroscedasticity           
  H0: The variance of e is the same for all values of x.           
  H1: The variance of e is not the same for all values of x.         
  According to the plot, there is no sign of heteroscedasticity.        
    
  3.Dependence of the Error Variable         
  H0 : Randomness exists.        
  H1 : Randomness does not exist.      
  Since pvalue = 0.1424> 0.05, we do not reject H0.


*Assessment*                
  The standard error of estimate (3894) is samll compared to mean_y (33423.49 ). We can conclude that the model fit the data.
  
  r^2 = 82.33% of the variation in the fund value is explained by the variation in Des_x. The rest(17.67%) remains unexplained by this model.        
  
  Testing the coefficients beta1.      
  H0: beta1 = 0, H1: beta1 != 0          
  Since p-value <2e-16 , we are 95% confident to reject H0. There is overwhelming evidence that t affects Des_x.       

-------------------------------------------------------------------------------------------

The # of visitors forecast for the next 12 months are 30946.44, 47064.33, 61375.70, 50128.48, 46502.68, 42773.13, 29861.63, 35593.17, 46384.51, 51469.57, 69428.68, 81420.85 respectively.



## Smoothing by Linear Regression Model
```{r}
SLR_SI <- Linear_Regression_Seasonal_Index(Visitors, Quarter, Season)
cat("# # # # The Simple Linear_Regression seasonal index # # # ","\n")
print(SLR_SI)
```

### Forecast by seasonal index
```{r}
Forecast_Visitors_SLR <- Forecast_by_SI(Visitors, Quarter, Season, SLR_SI, t)

cat("# # # # The Forecast_Visitors_y # # # ","\n")
print(Forecast_Visitors_SLR)
#write.csv(Forecast_Visitors_SLR,"../../temp.csv")

lines(t, Visitors, type="o", col = "blue")    #actual
lines(t2,Forecast_Visitors_SLR, type="o", col = "red")  #forecast
legend("topleft", c("actual", "deseasonalized", "forecast"), fill=c("blue","green", "red"))
```

*Residual analysis: *            
  1.normality test       
  H0: Errors are normally distributed.        
  H1: Errors are not normally distributed.         
  Since pvalue = 0.4103 > 0.05, we do not reject H0.
  
  2..Homoscedasticity & Heteroscedasticity           
  H0: The variance of e is the same for all values of x.           
  H1: The variance of e is not the same for all values of x.         
  According to the plot, there is no sign of heteroscedasticity.        
    
  3.Dependence of the Error Variable         
  H0 : Randomness exists.        
  H1 : Randomness does not exist.      
  Since pvalue = 0.1424> 0.05, we do not reject H0.


*Assessment*                
  The standard error of estimate (3922) is samll compared to mean_y (33452.34). We can conclude that the model fit the data.
  
  r^2 = 82.06% of the variation in the fund value is explained by the variation in Des_x. The rest(17.94%) remains unexplained by this model.        
  
  Testing the coefficients beta1.      
  H0: beta1 = 0, H1: beta1 != 0          
  Since p-value <2e-16 , we are 95% confident to reject H0. There is overwhelming evidence that t affects Des_x.       

-------------------------------------------------------------------------------------------

The # of visitors forecast for the next 12 months are 31189.49, 47780.60, 61563.53, 50174.54, 44918.51, 41675.48, 29818.83, 35430.62, 46955.87, 51187.61, 70204.90, 82041.74 respectively.

  


## Regression model by indicator variables

### Line Chart by Quarters on excel
All the # of visitors for the twelve quarters have positive linear relationship with time.


### The Regression Model
y = beta0 + beta1\*t + beta2\*Q1 + beta3\*Q2 + beta4\*Q3 + beta5\*Q4 +beta6\*Q5 +beta7\*Q6 +beta8\*Q7 +beta\*9Q8 +beta10\*Q9 +beta11\*Q10 +vbeta12Q\*11 +e
where
t = time in chronological order
Qi = indicator variable (0, 1).

### Dummy as Quarter
```{r}
Dummy_I <- Season - 1    # n-1 indicator variables

Q <- matrix(nrow =n, ncol = Dummy_I)
Dummy_Name <- vector("character", length = Dummy_I)

for(i in 1:Dummy_I){
  Dummy_Name[i] <- paste0("Q", i)
}

colnames(Q) <- Dummy_Name

for(i in 1:n){
  for(j in 1:Dummy_I){
    if(Quarter[i] == j){
      Q[i,j] <- 1
    } else {
      Q[i,j] <- 0
    }  
  }
}
```

### build the model
```{r}
Data.Visitors <- cbind(t, Q)  #t, Q1, Q2, Q3...
linearModelVar1 <- lm(Visitors ~ Data.Visitors)
cat("\n# # # # The regression model # # # ","\n")
print(summary(linearModelVar1))
cat(" ","\n")
ANOVA_T_lm <- anova(linearModelVar1)
cat("# # # # The ANOVA Table # # # ","\n")
print(ANOVA_T_lm)


#residual analysis 
SE_1 <- standarized_errors_MR(Visitors, Data.Visitors)[,1]
h_1 <- standarized_errors_MR(Visitors, Data.Visitors)[,2]
D_1 <- standarized_errors_MR(Visitors, Data.Visitors)[,3]
print(shapiro.test(SE_1) )          # normality
b0<- coef(linearModelVar1)[1]             # Homoscedasticity & Heteroscedasticity 
b1 <- coef(linearModelVar1)[2]
b2 <- coef(linearModelVar1)[3]
b3 <- coef(linearModelVar1)[4]
b4 <- coef(linearModelVar1)[5]
b5 <- coef(linearModelVar1)[6]
b6 <- coef(linearModelVar1)[7]
b7 <- coef(linearModelVar1)[8]
b8 <- coef(linearModelVar1)[9]
b9 <- coef(linearModelVar1)[10]
b10 <- coef(linearModelVar1)[11]
b11 <- coef(linearModelVar1)[12]
b12 <- coef(linearModelVar1)[13]

yhat <- vector("double", length = n)
for(i in 1:n) {
       yhat <- b0 + b1 *t + b2 *Q[i,1] + b3 *Q[i,2] + b4 *Q[i,3]+ b5 *Q[i,4]+ b6 *Q[i,5]+ b7 *Q[i,6]+ b8 *Q[i,7]+ b9 *Q[i,8]+ b10 *Q[i,9]+ b11 *Q[i,10]+ b12 *Q[i,11]
}
plot(x = yhat, y = SE_1, xlab = "Predicted visitors", ylab = "Standardized Error", main = "predicted visitors vs Error")
cat("\nRun_Test:\n")
print(Run_Test(SE_1))              #errors are independent


cat("\nThe Outliers\n")
Outliers <- abs(SE_1) > 2
if(sum(Outliers)!=0){
    print(which(Outliers))
}else{
  print("no outliers")
}  
cat("\ninfluential observations:\n")

k <- dim(Data.Visitors)[2]
Inf_Obs <- h_1 > 3*(k+1)/n
if(sum(Inf_Obs)!=0){
    print(which(Inf_Obs))
}else{
  print("no influential observations")
} 


#assesment
cat("\nmean_y:",mean(Visitors),"\n")
```
*Residual analysis: *            
  1.normality test       
  H0: Errors are normally distributed.        
  H1: Errors are not normally distributed.         
  Since pvalue = 0.7282 > 0.05, we do not reject H0.
  
  2..Homoscedasticity & Heteroscedasticity           
  H0: The variance of e is the same for all values of x.           
  H1: The variance of e is not the same for all values of x.         
  According to the plot, there is no sign of heteroscedasticity.        
    
  3.Dependence of the Error Variable         
  H0 : Randomness exists.        
  H1 : Randomness does not exist.      
  Since pvalue = 0.0667 > 0.05, we do not reject H0.


*Assessment*                
  The standard error of estimate (4566) is samll compared to mean_y (33560.08 ). We can conclude that the model fit the data.
  
  r^2 = 90.25% of the variation in the fund value is explained by the variation in Des_x. The rest(9.75%) remains unexplained by this model.        
  
  The F-test of ANOVA:
H0: beta1 = beta2 = â€¦ = beta11 = 0
H1: At least one betai is not equal to zero.
` since p-value < 2.2e-16 < 0.05, there is sufficient evidence to reject the null hypothesis. At least one of the betai is not equal to zero. Thus, at least one independent variable is related to y. This regression model is valid.

  Testing of the Coefficients 
H0: betai = 0
H1: betai != 0
` Since all the pvalue < 0.05, there is overwhelming evidence to infer that all the Qi and t affects # of visitors. 
  
  
### Forecast
```{r}
NoB <- Dummy_I + 2  #b0 ,b1(t), b2(Q1), b3(Q2), b4(Q3)...
bi <- vector("double", length = NoB)
for(i in 1:NoB){
  bi[i] <- coef(linearModelVar1)[i]  #bi[1]:intercept, bi[2]: coefficient of t
}
b0 <- vector("double", length = n)
for(j in 1:n) {
    b0[j] <- 1
}

Data.Visitors_0 <- cbind(b0, Data.Visitors) # matrix[n*NoB]: each bi for all the n samples
y_Dummy <- c(Data.Visitors_0 %*% bi)     # bi[NoB*1]: 1 (*b0), t (*b1), 1or0 (*b2), 1or0 (*b3)...  #only 24 periods

#the next 12 months for 
Q1 = bi[1] + bi[2]*120 + bi[3]
Q2 = bi[1] + bi[2]*121 + bi[4] 
Q3 = bi[1] + bi[2]*122 + bi[5]
Q4 = bi[1] + bi[2]*123 + bi[6]
Q5 = bi[1] + bi[2]*124 + bi[7]
Q6 = bi[1] + bi[2]*125 + bi[8]
Q7 = bi[1] + bi[2]*126 + bi[9]
Q8 = bi[1] + bi[2]*127 + bi[10]
Q9 = bi[1] + bi[2]*128 + bi[11]
Q10 = bi[1] + bi[2]*129+ bi[12]
Q11 = bi[1] + bi[2]*130 + bi[13]
Q12 =  bi[1] + bi[2]*131   ###

y_Dummy = c(y_Dummy,Q1,Q2,Q3,Q4,Q5,Q6,Q7,Q8,Q9,Q10,Q11,Q12)  #28 periods total

cat("# # # # The Forecast_Visitors_y# # # ","\n")
print(y_Dummy)  


t2 = c(t,n,n+1,n+2,n+3,n+4,n+5,n+6,n+7,n+8,n+9,n+10,n+11)
plot(t,Visitors, col = "blue", main = "Visitors against Period", xlab = "Period", ylab = "Vistitors", xlim=range(t2, t2+1), ylim=range(y_Dummy,0))
lines(t, Visitors, type="o", col = "blue")    #actual
lines(t2,y_Dummy, type="o", col = "red")  #forecast
abline(lm(Visitors ~ t))
legend("topleft", c("actual", "forecast"), fill=c("blue","red"))
```  

The # of visitors forecast for the next 12 months are 36159.160, 47791.260, 58047.660, 49537.460, 46656.062, 44837.362, 36609.062, 40033.562, 47883.762, 51132.062, 63723.162, 71835.962.
 
 
 
## Error Metrics
```{r}
CMV_MAD <- Mean_Absolute_Deviation(Visitors, Forecast_Visitors_CMV, 1)  #actual y, predicted y
CMV_MSE <- Mean_Square_Error(Visitors, Forecast_Visitors_CMV, 1)
CMV_MAPE <- Mean_Absolute_Percentage_Error(Visitors, Forecast_Visitors_CMV, 1)
SLR_MAD <- Mean_Absolute_Deviation(Visitors, Forecast_Visitors_SLR, 1)
SLR_MSE <- Mean_Square_Error(Visitors, Forecast_Visitors_SLR, 1)
SLR_MAPE <- Mean_Absolute_Percentage_Error(Visitors, Forecast_Visitors_SLR, 1)
Dummy_MAD <- Mean_Absolute_Deviation(Visitors, y_Dummy, 1)
Dummy_MSE <- Mean_Square_Error(Visitors, y_Dummy, 1)
Dummy_MAPE <- Mean_Absolute_Percentage_Error(Visitors, y_Dummy, 1)
Error_M <- matrix(nrow = 3, ncol = 3)
colnames(Error_M) = c("CMV", "SLR", "Dummy")
rownames(Error_M) = c("MAD", "MSE", "MAPE")
Error_M[1,1] <- CMV_MAD
Error_M[1,2] <- SLR_MAD
Error_M[1,3] <- Dummy_MAD
Error_M[2,1] <- CMV_MSE
Error_M[2,2] <- SLR_MSE
Error_M[2,3] <- Dummy_MSE
Error_M[3,1] <- CMV_MAPE
Error_M[3,2] <- SLR_MAPE
Error_M[3,3] <- Dummy_MAPE
cat("\n# # # # Error Metrix # # # ","\n")
print(Error_M)
```
MAD: SLR is smaller      
MSE: SLR is smaller       
MAPE: SLR is smaller       
Therefore, we choose SLR as the best method to forecast the # of visitors from Malaysia for the next 12 momnths.   

-------------------------------------------------------------------------------------------------

##Conclusion:
By SLR, we forecast that the number of visitors from Malaysia for the next 12 months will be  31189.49, 47780.60, 61563.53, 50174.54, 44918.51, 41675.48, 29818.83, 35430.62, 46955.87, 51187.61, 70204.90, 82041.74 respectively.



